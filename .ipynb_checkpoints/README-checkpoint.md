# Adaptive Momentum Gradient Descent for Regularized Poisson Regression

**A Novel Optimization Algorithm with Adaptive Soft-Thresholding**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Research Paper](https://img.shields.io/badge/paper-published-brightgreen.svg)](https://github.com/elbakari01/amgd_poisson_regression)

## üìã Overview

This repository contains the complete implementation and experimental validation of **Adaptive Momentum Gradient Descent (AMGD)**, a novel optimization algorithm for regularized Poisson regression developed by Ibrahim Bakari and M. Revan √ñzkale from √áukurova University.

AMGD addresses critical limitations of existing optimizers (AdaGrad's rapid learning rate decay, Adam's gradient instability in sparse data) by integrating adaptive learning rates, momentum updates, gradient clipping, and **adaptive soft-thresholding** into a unified framework.

## üéØ Key Research Contributions

### Novel AMGD Algorithm Features:
- **Adaptive Soft-Thresholding**: `Œ≤‚±º = sign(Œ≤‚±º) √ó max(|Œ≤‚±º| - Œ±‚ÇúŒª‚ÇÅ/(|Œ≤‚±º| + Œµ), 0)`
- **Dual-Level Gradient Clipping**: Prevents numerical instability from exponential link function
- **Momentum with Bias Correction**: Enhanced convergence properties
- **Adaptive Learning Rate Decay**: `Œ±‚Çú = Œ±/(1 + Œ∑t)` for guaranteed convergence

### Theoretical Guarantees:
- **Convergence Rate**: O(1/‚àöT) after T iterations under convexity
- **Feature Selection Optimality**: Proven oracle properties for L1 regularization
- **Numerical Stability**: Robust to heavy-tailed gradients in sparse count data

## üìä Performance Results

### Experimental Dataset:
- **Ecological Health Dataset**: 61,345 observations, 17 features
- **Target**: Biodiversity Index (count data, Poisson-distributed)
- **Features**: Environmental indicators (air quality, soil characteristics, water metrics)

### Performance Comparison:
| Optimizer | MAE ‚Üì  | RMSE ‚Üì | MPD ‚Üì  | Sparsity ‚Üë | Runtime (s) |
|-----------|--------|--------|--------|------------|-------------|
| **AMGD**  | **2.985** | **3.873** | **2.188** | **29.29%** | **0.002** |
| Adam      | 3.081  | 3.983  | 2.225  | 11.76%     | 0.004 |
| AdaGrad   | 6.862  | 7.579  | 10.965 | 5.00%      | 0.745 |
| GLMnet    | 9.007  | 9.551  | 28.848 | 52.93%     | 0.040 |

### Key Improvements:
- **56.6% better MAE** compared to AdaGrad
- **2.7% better MAE** compared to Adam  
- **Superior sparsity induction** (29.29% vs Adam's 11.76%)
- **Fastest computational efficiency** (0.002s runtime)

## üî¨ Statistical Significance

All improvements are statistically significant with **p < 0.0001**:

| Comparison | MAE | RMSE | Mean Deviance | Effect Size (Cohen's d) |
|------------|-----|------|---------------|------------------------|
| AMGD vs Adam | p<0.0001 | p<0.0001 | p<0.0001 | d=-1.33* |
| AMGD vs AdaGrad | p<0.0001 | p<0.0001 | p<0.0001 | d=-9.58* |
| AMGD vs GLMnet | p<0.0001 | p<0.0001 | p<0.0001 | d=-10.78* |

*Large effect sizes indicate substantial practical significance

## üöÄ Quick Start

### 1. Clone Repository
```bash
git clone https://github.com/yourusername/amgd-poisson-regression-research.git
cd amgd-poisson-regression-research
```

### 2. Setup Environment
```bash
pip install -r requirements.txt
```

### 3. Run Complete Analysis
```bash
python amgd_implementation.py
```

### 4. Quick Test (5 minutes)
```bash
python amgd_implementation.py --quick --optimizers AMGD Adam
```

## üîç Algorithm Details

### Mathematical Formulation

**1. Poisson Log-Likelihood:**
```
L(Œ≤) = -‚àë[y·µ¢ log(Œº·µ¢) - Œº·µ¢ - log(y·µ¢!)]
where Œº·µ¢ = exp(x·µ¢·µÄŒ≤)
```

**2. AMGD Update Rules:**
```
# Momentum Updates
m‚Çú = Œ∂‚ÇÅm‚Çú‚Çã‚ÇÅ + (1-Œ∂‚ÇÅ)‚àáL
v‚Çú = Œ∂‚ÇÇv‚Çú‚Çã‚ÇÅ + (1-Œ∂‚ÇÇ)[‚àáL]¬≤

# Bias Correction  
mÃÇ‚Çú = m‚Çú/(1-Œ∂‚ÇÅ·µó), vÃÇ‚Çú = v‚Çú/(1-Œ∂‚ÇÇ·µó)

# Parameter Update
Œ≤ = Œ≤ - Œ±‚ÇúmÃÇ‚Çú/(‚àövÃÇ‚Çú + Œµ)

# Adaptive Soft-Thresholding
Œ≤‚±º = sign(Œ≤‚±º) √ó max(|Œ≤‚±º| - Œ±‚ÇúŒª‚ÇÅ/(|Œ≤‚±º| + Œµ), 0)
```

### Key Innovations:

**1. Adaptive Soft-Thresholding:**
- Traditional: Fixed threshold Œª
- AMGD: Adaptive threshold `Œ±‚ÇúŒª‚ÇÅ/(|Œ≤‚±º| + Œµ)`
- **Benefit**: Large coefficients preserved, small coefficients aggressively shrunk

**2. Gradient Clipping:**
- **Element-wise clipping**: `clip(g‚±º) = max(-T, min(g‚±º, T))`
- **Linear predictor clipping**: Prevents exponential overflow
- **Critical for Poisson regression** due to exponential link function

**3. Convergence Guarantees:**
- **Rate**: O(1/‚àöT) convergence to optimal solution
- **Conditions**: Diminishing step sizes `‚àëŒ±‚Çë = ‚àû, ‚àëŒ±‚Çú¬≤ < ‚àû`
- **Stability**: Robust to sparse, high-dimensional data

## üìÅ Repository Structure

```
amgd-poisson-regression-research/
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ amgd_implementation.py              # Complete research implementation
‚îú‚îÄ‚îÄ requirements.txt                    # Exact dependency versions
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ ecological_health_dataset.csv   # Biodiversity dataset (61,345 obs)
‚îÇ   ‚îî‚îÄ‚îÄ README.md                       # Data description
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ figures/                        # Generated plots and visualizations
‚îÇ   ‚îú‚îÄ‚îÄ tables/                         # Numerical results (CSV format)
‚îÇ   ‚îî‚îÄ‚îÄ reports/                        # Experimental summaries
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ algorithm_explanation.md        # Mathematical details
‚îÇ   ‚îú‚îÄ‚îÄ experimental_setup.md           # Complete methodology
‚îÇ   ‚îî‚îÄ‚îÄ theoretical_analysis.md         # Convergence proofs
‚îî‚îÄ‚îÄ reproduction/
    ‚îú‚îÄ‚îÄ reproduction_guide.md           # Step-by-step reproduction
    ‚îî‚îÄ‚îÄ troubleshooting.md              # Common issues and solutions
```

## üî¨ Experimental Design

### Cross-Validation Setup:
- **Strategy**: 5-fold cross-validation  
- **Data Split**: 70% training, 15% validation, 15% test
- **Hyperparameter Search**: 50 Œª values (10‚Åª¬≥ to 10¬π)
- **Regularization**: L1 (Lasso) and ElasticNet

### Performance Metrics:
1. **Mean Absolute Error (MAE)** - Primary metric
2. **Root Mean Squared Error (RMSE)** - Secondary metric  
3. **Mean Poisson Deviance (MPD)** - Distributional fit quality
4. **Sparsity** - Feature selection capability
5. **Runtime** - Computational efficiency

### Statistical Validation:
- **Bootstrap Analysis**: 1000 resamples for confidence intervals
- **Significance Testing**: Paired t-tests with Bonferroni correction
- **Effect Size**: Cohen's d for practical significance
- **Feature Selection Stability**: Across bootstrap samples

## üìà Real-World Application: Ecological Modeling

### Dataset Description:
- **Domain**: Environmental science and biodiversity conservation
- **Size**: 61,345 observations across diverse ecosystems
- **Features**: 
  - **Environmental**: Temperature, humidity, precipitation, vegetation coverage
  - **Pollution**: Air quality index, PM2.5, soil/water contamination
  - **Categorical**: Pollution levels, ecological health classifications
- **Target**: Biodiversity Index (species count per ecosystem)

### Scientific Impact:
- **Conservation Planning**: Identify key environmental drivers
- **Policy Development**: Quantify pollution impact on biodiversity  
- **Predictive Modeling**: Early warning systems for ecosystem degradation
- **Resource Allocation**: Prioritize conservation efforts

### Key Findings:
- **Ecological health indicators** most predictive (100% selection probability)
- **Pollution levels** consistently selected across bootstrap samples
- **Environmental variables** show moderate selection frequencies (53-70%)
- **Sparse models** (29% features) maintain predictive accuracy

## üîÑ Reproducibility

### Complete Reproducibility Package:
- **Fixed Random Seed**: 42 throughout all experiments
- **Exact Dependencies**: Pinned versions in requirements.txt
- **Statistical Validation**: Bootstrap confidence intervals
- **Cross-Platform**: Tested on Windows/macOS/Linux

### Expected Runtime:
- **Quick Test**: 2-5 minutes
- **Full Analysis**: 1-2 hours
- **Statistical Validation**: Additional 30 minutes

### Validation Metrics:
Your results should show:
- AMGD outperforming other methods on MAE/RMSE
- Statistical significance p < 0.05 for key comparisons
- Sparsity levels between 20-35% for AMGD
- Convergence within 10-20% of maximum iterations

## üìö Citation

If you use this work in your research, please cite:

```bibtex
@article{bakari2024amgd,
  title={Adaptive Momentum Gradient Descent: A New Algorithm in Regularized Poisson Regression},
  author={Bakari, Ibrahim and √ñzkale, M. Revan},
  journal={[Journal Name]},
  year={2024},
  publisher={[Publisher]},
}
```

## üîó Related Work

- **Kingma & Ba (2014)**: Adam optimizer foundation
- **Duchi et al. (2011)**: AdaGrad adaptive learning rates  
- **Friedman et al. (2010)**: GLMnet coordinate descent
- **Zou (2006)**: Adaptive Lasso oracle properties
- **Tibshirani (1996)**: Original Lasso formulation

## üë• Authors

**Ibrahim Bakari** (Corresponding Author) 
- Department of Statistics, Faculty of Science and Letters
- √áukurova University, Adana, 01330, T√ºrkiye
- Email: 2020913072@ogr.cu.edu.tr/acbrhmbakari@gmail.com

**M. Revan √ñzkale** 
- Department of Statistics, Faculty of Science and Letters  
- √áukurova University, Adana, 01330, T√ºrkiye
- Email: mrevan@cu.edu.tr

## ü§ù Contributing

We welcome contributions to improve the algorithm or extend the analysis:

1. **Algorithm Improvements**: Stochastic variants, distributed implementations
2. **Theoretical Extensions**: Non-convex convergence analysis
3. **Applications**: Additional datasets and domains
4. **Implementation**: GPU acceleration, online learning variants

## üìÑ License

This research implementation is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- √áukurova University for research support
- Ecological data providers
- Open source community for optimization libraries
- Reviewers and collaborators for valuable feedback

---

‚≠ê **Star this repository if you find this research useful!** ‚≠ê

üîÑ **Fork it to build upon this work!** 

üìß **Contact us for academic collaborations!**

üìä **Use our algorithm in your research and cite our work!**