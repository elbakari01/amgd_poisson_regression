# Research Methodology: AMGD for Regularized Poisson Regression

## Table of Contents
1. [Research Framework](#research-framework)
2. [Algorithm Development](#algorithm-development)
3. [Theoretical Analysis](#theoretical-analysis)
4. [Experimental Design](#experimental-design)
5. [Statistical Analysis](#statistical-analysis)
6. [Validation Framework](#validation-framework)
7. [Reproducibility Protocol](#reproducibility-protocol)

## Research Framework

### Research Questions
1. **Primary Question**: Can a unified optimization framework combining adaptive learning rates, momentum, gradient clipping, and soft-thresholding outperform existing methods for regularized Poisson regression?

2. **Secondary Questions**:
   - How does adaptive soft-thresholding compare to traditional fixed thresholding?
   - What is the impact of gradient clipping on Poisson regression optimization stability?
   - Can embedded feature selection during optimization improve both sparsity and accuracy?

### Research Hypotheses
- **H₁**: AMGD achieves superior predictive accuracy compared to Adam, AdaGrad, and GLMnet
- **H₂**: AMGD provides better sparsity-accuracy trade-offs through adaptive thresholding
- **H₃**: Gradient clipping in AMGD improves numerical stability for Poisson regression
- **H₄**: AMGD demonstrates faster convergence to optimal solutions

### Research Design
- **Type**: Comparative experimental study
- **Approach**: Quantitative analysis with statistical validation
- **Scope**: Single large-scale dataset with comprehensive evaluation
- **Validation**: Bootstrap resampling and cross-validation

## Algorithm Development

### Design Principles
1. **Unified Framework**: Integrate multiple optimization techniques cohesively
2. **Adaptive Mechanisms**: Dynamic adjustment based on parameter magnitudes
3. **Numerical Stability**: Robust to extreme values in Poisson regression
4. **Embedded Selection**: Feature selection during optimization process
5. **Theoretical Grounding**: Convergence guarantees and oracle properties

### Component Integration Strategy

#### 1. Momentum with Adaptive Scaling
**Rationale**: Accelerate convergence while maintaining stability
```
mₜ = ζ₁mₜ₋₁ + (1-ζ₁)∇f(βₜ)
vₜ = ζ₂vₜ₋₁ + (1-ζ₂)[∇f(βₜ)]²
```

**Innovation**: Bias correction for initialization effects
```
m̂ₜ = mₜ/(1-ζ₁ᵗ), v̂ₜ = vₜ/(1-ζ₂ᵗ)
```

#### 2. Adaptive Learning Rate Decay
**Rationale**: Ensure convergence while maintaining exploration
```
αₜ = α/(1 + ηt)
```

**Theoretical Basis**: Satisfies Robbins-Monro conditions for convergence

#### 3. Dual-Level Gradient Clipping
**Element-wise Clipping**:
```
grad_clipped = clip(grad, -T, T)
```

**Linear Predictor Clipping**:
```
linear_pred = clip(Xβ, -20, 20)
```

**Rationale**: Prevent exponential overflow in Poisson link function

#### 4. Adaptive Soft-Thresholding
**Traditional Approach**:
```
prox_λ(z) = sign(z) × max(|z| - λ, 0)
```

**AMGD Innovation**:
```
βⱼ = sign(βⱼ) × max(|βⱼ| - αₜλ₁/(|βⱼ| + ε), 0)
```

**Advantages**:
- Large coefficients: Lighter penalization
- Small coefficients: Aggressive shrinkage
- Dynamic adaptation: No staged reweighting

### Algorithm Architecture

```python
class AMGD:
    def __init__(self, alpha=0.05, zeta1=0.9, zeta2=0.999, 
                 lambda1=0.1, lambda2=0.0, penalty='l1',
                 T=10.0, eta=1e-4, epsilon=1e-8):
        # Initialize hyperparameters
        
    def _clip_gradients(self, gradients, threshold):
        # Element-wise gradient clipping
        
    def _adaptive_soft_threshold(self, beta, alpha_t, lambda1):
        # Coefficient-dependent thresholding
        
    def _update_moments(self, gradients, t):
        # Momentum and second moment updates with bias correction
        
    def fit(self, X, y, max_iter=1000, tol=1e-6):
        # Main optimization loop
```

## Theoretical Analysis

### Convergence Theory

#### Problem Setup
Consider the composite objective function:
```
f(β) = g(β) + h(β) = -ℓ(β) + λP(β)
```

Where:
- `g(β) = -ℓ(β)` is the smooth negative log-likelihood
- `h(β) = λP(β)` is the non-smooth regularization term

#### Assumptions
1. **Smoothness**: `g(β)` has L-Lipschitz continuous gradient
2. **Convexity**: `f(β)` is convex (under certain design matrix conditions)
3. **Bounded Gradients**: `∥∇g(β)∥∞ ≤ G` (ensured by clipping)
4. **Valid Momentum**: `0 ≤ ζ₁, ζ₂ < 1`

#### Main Convergence Result
**Theorem 1**: Under the above assumptions and step size conditions:
```
∑αₜ = ∞, ∑αₜ² < ∞
```

The sequence {βₜ} generated by AMGD converges to optimal solution β* at rate O(1/√T).

#### Proof Sketch
1. **Lyapunov Function**: Define Ψₜ = f(βₜ) + (1/2αₜ)∥βₜ - β*∥²
2. **Descent Property**: Show Ψₜ₊₁ ≤ Ψₜ under appropriate conditions
3. **Convergence**: Use diminishing step sizes and bounded gradients

### Feature Selection Theory

#### Oracle Properties
**Proposition 1**: For Poisson regression with L1 regularization, the optimal feature subset S* minimizes:
```
S* = argmin_{S⊆{1,...,p}} E[L(y, f_S(x))] + α|S|
```

#### Adaptive Lasso Connection
AMGD's adaptive thresholding mimics the Adaptive Lasso weights:
```
wⱼ = 1/|βⱼ|^γ
```

But with dynamic updates eliminating staged reweighting.

### Computational Complexity

#### Per-Iteration Cost
- **Matrix-Vector Multiplication**: O(np) for X^T(μ - y)
- **Momentum Updates**: O(p)
- **Soft-Thresholding**: O(p)
- **Total**: O(np) - optimal for full-batch methods

#### Memory Requirements
- **Parameters**: O(p) for β, m, v
- **Gradients**: O(p) temporary storage
- **Data**: O(np) for design matrix
- **Total**: O(np) - linear scaling

## Experimental Design

### Dataset Selection Criteria
1. **Size**: Large enough for statistical power (> 50,000 observations)
2. **Dimensionality**: Sufficient features for sparsity evaluation (> 15 features)
3. **Count Nature**: Poisson-distributed target variable
4. **Real-World Relevance**: Practical application domain
5. **Feature Diversity**: Mix of continuous and categorical variables

### Ecological Health Dataset Justification
- **Domain Relevance**: Environmental conservation applications
- **Statistical Properties**: Count data with natural Poisson characteristics
- **Feature Richness**: Environmental measurements and categorical indicators
- **Scientific Impact**: Biodiversity prediction for conservation planning
- **Data Quality**: Large sample size with comprehensive feature coverage

### Experimental Protocol

#### 1. Data Preprocessing
```python
def preprocess_data(df):
    # Handle missing values (median/mode imputation)
    # Standardize continuous variables (z-score normalization)
    # One-hot encode categorical variables (drop reference)
    # Apply outlier treatment (IQR-based winsorization)
    return X, y, feature_names
```

#### 2. Train-Validation-Test Split
```
Training:   70% (42,939 obs) - Model fitting
Validation: 15% (9,204 obs)  - Hyperparameter tuning  
Test:       15% (9,202 obs)  - Final evaluation
```

#### 3. Cross-Validation Framework
```python
def cross_validate(X_val, y_val, optimizer, param_grid):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = []
    for train_idx, val_idx in kf.split(X_val):
        # Train on fold
        # Evaluate on held-out portion
        # Record performance metrics
    return np.mean(scores), np.std(scores)
```

#### 4. Hyperparameter Optimization
```python
param_grids = {
    'AMGD': {
        'alpha': [0.01, 0.05, 0.1],
        'eta': [1e-5, 1e-4, 1e-3],
        'T': [5, 10, 20],
        'lambda': np.logspace(-3, 1, 50)
    },
    # Similar grids for other optimizers
}
```

### Performance Evaluation Framework

#### Primary Metrics
1. **Mean Absolute Error (MAE)**
   - Robust to outliers in count data
   - Primary optimization criterion
   - Direct interpretability

2. **Root Mean Squared Error (RMSE)**
   - Standard regression metric
   - Penalizes large errors more heavily
   - Comparison baseline

3. **Mean Poisson Deviance (MPD)**
   - Distribution-specific metric
   - Accounts for Poisson likelihood
   - Theoretical foundation

#### Secondary Metrics
4. **Sparsity Percentage**
   - Feature selection capability
   - Model interpretability measure
   - Practical importance

5. **Computational Runtime**
   - Efficiency comparison
   - Scalability assessment
   - Practical considerations

#### Convergence Analysis
- **Objective Function**: Monitor total loss (likelihood + penalty)
- **Iteration Counts**: Convergence speed comparison
- **Stability**: Consistency across runs

## Statistical Analysis

### Hypothesis Testing Framework

#### 1. Performance Comparison
**Test**: Paired t-tests comparing AMGD vs each baseline
```
H₀: μ_AMGD = μ_baseline
H₁: μ_AMGD < μ_baseline (for error metrics)
```

**Multiple Comparisons**: Bonferroni correction
```
α_adjusted = α / number_of_tests
```

#### 2. Effect Size Calculation
**Cohen's d**: Standardized effect size
```
d = (μ₁ - μ₂) / σ_pooled
```

**Interpretation**:
- Small: |d| ≈ 0.2
- Medium: |d| ≈ 0.5  
- Large: |d| ≈ 0.8+

#### 3. Bootstrap Confidence Intervals
```python
def bootstrap_ci(data, n_bootstrap=1000, ci_level=0.95):
    bootstrap_means = []
    for _ in range(n_bootstrap):
        sample = np.random.choice(data, size=len(data), replace=True)
        bootstrap_means.append(np.mean(sample))
    
    alpha = 1 - ci_level
    lower = np.percentile(bootstrap_means, 100 * alpha/2)
    upper = np.percentile(bootstrap_means, 100 * (1 - alpha/2))
    return lower, upper
```

### Feature Selection Analysis

#### Selection Stability
```python
def feature_stability(X, y, optimizer, n_bootstrap=1000):
    selection_counts = np.zeros(X.shape[1])
    
    for i in range(n_bootstrap):
        # Bootstrap sample
        indices = np.random.choice(len(X), size=len(X), replace=True)
        X_boot, y_boot = X[indices], y[indices]
        
        # Fit model
        beta = optimizer.fit(X_boot, y_boot)
        
        # Count selections
        selected = np.abs(beta) > 1e-6
        selection_counts += selected
    
    return selection_counts / n_bootstrap
```

#### Selection Probability Ranking
- Rank features by selection frequency
- Identify consistently selected features
- Assess selection stability across methods

### Robustness Analysis

#### 1. Sensitivity to Hyperparameters
- Test performance across parameter ranges
- Identify robust parameter regions  
- Assess tuning requirements

#### 2. Convergence Stability
- Multiple random initializations
- Convergence rate variability
- Final solution consistency

#### 3. Data Perturbation
- Bootstrap resampling stability
- Outlier sensitivity analysis
- Missing data robustness

## Validation Framework

### Internal Validation

#### 1. Cross-Validation
- **Purpose**: Hyperparameter optimization and model selection
- **Method**: 5-fold stratified cross-validation
- **Metric**: Mean Absolute Error (primary)
- **Robustness**: Multiple random seeds

#### 2. Bootstrap Validation
- **Purpose**: Performance stability assessment
- **Method**: 1000 bootstrap resamples
- **Output**: Confidence intervals for all metrics
- **Analysis**: Distribution shape and variance

#### 3. Convergence Validation
- **Objective Monitoring**: Track total loss convergence
- **Gradient Norms**: Monitor optimization progress
- **Parameter Stability**: Check solution consistency

### External Validation

#### 1. Hold-Out Test Set
- **Size**: 15% of total data (9,202 observations)
- **Purpose**: Unbiased performance evaluation
- **Protection**: Never used during development/tuning
- **Analysis**: Final model comparison

#### 2. Statistical Significance
- **Tests**: Paired t-tests with multiple comparison correction
- **Effect Sizes**: Cohen's d for practical significance
- **Confidence**: Bootstrap confidence intervals

#### 3. Feature Selection Validation
- **Biological Plausibility**: Expert domain knowledge
- **Literature Consistency**: Comparison with existing studies
- **Selection Stability**: Across bootstrap samples

### Reproducibility Validation

#### 1. Computational Reproducibility
```python
# Set all random seeds
np.random.seed(42)
random.seed(42)
torch.manual_seed(42)  # if using PyTorch

# Document software versions
requirements = {
    'numpy': '1.21.0',
    'scipy': '1.7.0',
    'scikit-learn': '1.0.0',
    # ... other dependencies
}
```

#### 2. Platform Independence
- **Testing**: Windows, macOS, Linux validation
- **Numerical Precision**: Double precision (float64)
- **Library Versions**: Exact version specifications
- **Hardware**: CPU-only implementation for consistency

#### 3. Documentation Standards
- **Code Comments**: Comprehensive inline documentation
- **Parameter Descriptions**: Clear mathematical notation
- **Algorithm Steps**: Detailed pseudocode
- **Example Usage**: Complete working examples

## Reproducibility Protocol

### Pre-Experiment Setup

#### 1. Environment Configuration
```bash
# Create isolated environment
python -m venv amgd_env
source amgd_env/bin/activate

# Install exact versions
pip install -r requirements.txt

# Verify installation
python -c "import numpy; print(numpy.__version__)"
```

#### 2. Data Preparation
```python
# Standardized preprocessing pipeline
def preprocess_pipeline(raw_data):
    # Document each step
    # Apply in fixed order
    # Save intermediate results
    # Return processed data + metadata
```

#### 3. Random Seed Management
```python
RANDOM_SEED = 42

def set_seeds(seed=RANDOM_SEED):
    np.random.seed(seed)
    random.seed(seed)
    # Set any other library seeds
```

### Experiment Execution

#### 1. Hyperparameter Search
```python
def grid_search_cv(X, y, param_grid, cv_folds=5):
    results = []
    for params in param_grid:
        cv_scores = cross_validate(X, y, params, cv_folds)
        results.append({
            'params': params,
            'mean_score': np.mean(cv_scores),
            'std_score': np.std(cv_scores)
        })
    return sorted(results, key=lambda x: x['mean_score'])
```

#### 2. Model Training
```python
def train_model(X_train, y_train, best_params):
    model = AMGD(**best_params)
    
    # Track convergence
    loss_history = []
    start_time = time.time()
    
    model.fit(X_train, y_train, 
              callback=lambda loss: loss_history.append(loss))
    
    runtime = time.time() - start_time
    
    return model, loss_history, runtime
```

#### 3. Performance Evaluation
```python
def evaluate_all_metrics(models, X_test, y_test):
    results = {}
    for name, model in models.items():
        y_pred = model.predict(X_test)
        
        results[name] = {
            'MAE': mean_absolute_error(y_test, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
            'MPD': mean_poisson_deviance(y_test, y_pred),
            'Sparsity': calculate_sparsity(model.coef_),
            'Runtime': model.runtime_
        }
    
    return results
```

### Post-Experiment Analysis

#### 1. Statistical Testing
```python
def statistical_analysis(results_dict):
    # Paired t-tests
    pairwise_tests = {}
    for metric in ['MAE', 'RMSE', 'MPD']:
        pairwise_tests[metric] = {}
        
        amgd_scores = results_dict['AMGD'][metric]
        for baseline in ['Adam', 'AdaGrad', 'GLMnet']:
            baseline_scores = results_dict[baseline][metric]
            
            t_stat, p_val = ttest_rel(amgd_scores, baseline_scores)
            effect_size = cohens_d(amgd_scores, baseline_scores)
            
            pairwise_tests[metric][baseline] = {
                't_statistic': t_stat,
                'p_value': p_val,
                'effect_size': effect_size,
                'significant': p_val < 0.05
            }
    
    return pairwise_tests
```

#### 2. Results Documentation
```python
def generate_report(results, save_path):
    report = {
        'experiment_metadata': {
            'date': datetime.now().isoformat(),
            'random_seed': RANDOM_SEED,
            'software_versions': get_package_versions(),
            'hardware_info': get_system_info()
        },
        'performance_results': results,
        'statistical_tests': statistical_analysis(results),
        'feature_selection': analyze_feature_selection(results)
    }
    
    with open(save_path, 'w') as f:
        json.dump(report, f, indent=2)
```

#### 3. Validation Checks
```python
def validate_results(results):
    checks = {
        'amgd_best_mae': results['AMGD']['MAE'] < results['Adam']['MAE'],
        'statistical_significance': all(
            test['p_value'] < 0.05 
            for test in statistical_tests.values()
        ),
        'convergence_achieved': all(
            model.converged_ for model in trained_models.values()
        ),
        'sparsity_reasonable': 0.1 < results['AMGD']['Sparsity'] < 0.9
    }
    
    assert all(checks.values()), f"Validation failed: {checks}"
```

This comprehensive methodology ensures robust, reproducible, and statistically valid comparison of optimization algorithms for regularized Poisson regression, providing a solid foundation for scientific conclusions about AMGD's performance advantages.